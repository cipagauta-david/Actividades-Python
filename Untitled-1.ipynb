{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a45d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: El archivo 'readme.md' no fue encontrado.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def sanitizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Limpia una celda de texto eliminando formato Markdown,\n",
    "    saltos de l칤nea y espacios extra.\n",
    "    \"\"\"\n",
    "    # Elimina ** para negrita y * o _ para cursiva\n",
    "    texto = re.sub(r\"(\\*\\*|__|\\*|_)\", \"\", texto)\n",
    "    # Reemplaza <br> y saltos de l칤nea por un espacio\n",
    "    texto = texto.replace(\"<br>\", \" \").replace(\"\\n\", \" \")\n",
    "    # Elimina m칰ltiples espacios\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto)\n",
    "    return texto.strip()\n",
    "\n",
    "\n",
    "def analizar_linea_tabla_md(linea_md):\n",
    "    \"\"\"\n",
    "    Analiza una l칤nea de una tabla en formato Markdown y la divide en celdas.\n",
    "    \"\"\"\n",
    "    if not linea_md.startswith(\"|\"):\n",
    "        linea_md = \"|\" + linea_md\n",
    "    if not linea_md.endswith(\"|\"):\n",
    "        linea_md += \"|\"\n",
    "    celdas = [celda.strip() for celda in linea_md.split(\"|\")[1:-1]]\n",
    "    return celdas\n",
    "\n",
    "\n",
    "def extraer_y_transformar_tablas(contenido_md):\n",
    "    \"\"\"\n",
    "    Encuentra todas las tablas de casos de uso y las transforma en una lista de filas\n",
    "    para un CSV consolidado.\n",
    "\n",
    "    Args:\n",
    "        contenido_md (str): El contenido del archivo Markdown.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de listas, lista para ser escrita en un archivo CSV.\n",
    "    \"\"\"\n",
    "    lineas = contenido_md.split(\"\\n\")\n",
    "    datos_consolidados = [[\"id_caso_de_uso\", \"atributo\", \"valor\"]]\n",
    "    en_tabla = False\n",
    "    filas_tabla_actual = []\n",
    "\n",
    "    for linea in lineas:\n",
    "        # Detecta el inicio de una tabla por la l칤nea de separaci칩n (e.g., |:---|:---|)\n",
    "        if re.match(r\"^\\s*\\|?\\s*:?-+:?\\s*\\|\", linea):\n",
    "            if not en_tabla:\n",
    "                en_tabla = True\n",
    "                # La l칤nea anterior era el encabezado, pero para este formato\n",
    "                # no lo necesitamos ya que la tabla es clave-valor.\n",
    "                # Limpiamos las filas de la tabla anterior.\n",
    "                filas_tabla_actual = []\n",
    "                # La l칤nea anterior al separador es el encabezado de la tabla.\n",
    "                # La l칤nea anterior a esa puede ser el contenido de la primera fila.\n",
    "                # Asumimos que la tabla empieza 2 l칤neas antes de la de separaci칩n.\n",
    "                # Esta l칩gica se simplifica al procesar al final de la tabla.\n",
    "\n",
    "                # La l칤nea anterior es el encabezado, la guardamos\n",
    "                linea_encabezado = lineas[lineas.index(linea) - 1]\n",
    "                filas_tabla_actual.append(analizar_linea_tabla_md(linea_encabezado))\n",
    "                continue\n",
    "\n",
    "        if en_tabla:\n",
    "            # Si la l칤nea ya no parece parte de la tabla, la procesamos y cerramos\n",
    "            if not linea.strip().startswith(\"|\"):\n",
    "                en_tabla = False\n",
    "\n",
    "                # --- Procesar la tabla que acabamos de encontrar ---\n",
    "                if filas_tabla_actual and len(filas_tabla_actual[0]) == 2:\n",
    "                    # Asumimos que la primera fila siempre es el ID\n",
    "                    id_key = sanitizar_texto(filas_tabla_actual[0][0])\n",
    "                    if id_key == \"ID del Caso de Uso\":\n",
    "                        id_caso_de_uso = sanitizar_texto(filas_tabla_actual[0][1])\n",
    "\n",
    "                        # Recorremos todas las filas de esta tabla para a침adirlas\n",
    "                        for fila in filas_tabla_actual:\n",
    "                            if len(fila) == 2:\n",
    "                                atributo = sanitizar_texto(fila[0])\n",
    "                                valor = sanitizar_texto(fila[1])\n",
    "                                datos_consolidados.append(\n",
    "                                    [id_caso_de_uso, atributo, valor]\n",
    "                                )\n",
    "\n",
    "                filas_tabla_actual = []  # Limpiar para la siguiente tabla\n",
    "                continue\n",
    "\n",
    "            # Si seguimos en la tabla, a침adimos la fila\n",
    "            fila = analizar_linea_tabla_md(linea)\n",
    "            if len(fila) > 0:  # Ignorar l칤neas vac칤as\n",
    "                filas_tabla_actual.append(fila)\n",
    "\n",
    "    # Procesar la 칰ltima tabla si el archivo termina mientras se est치 leyendo\n",
    "    if en_tabla and filas_tabla_actual:\n",
    "        if len(filas_tabla_actual[0]) == 2:\n",
    "            id_key = sanitizar_texto(filas_tabla_actual[0][0])\n",
    "            if id_key == \"ID del Caso de Uso\":\n",
    "                id_caso_de_uso = sanitizar_texto(filas_tabla_actual[0][1])\n",
    "                for fila in filas_tabla_actual:\n",
    "                    if len(fila) == 2:\n",
    "                        atributo = sanitizar_texto(fila[0])\n",
    "                        valor = sanitizar_texto(fila[1])\n",
    "                        datos_consolidados.append([id_caso_de_uso, atributo, valor])\n",
    "\n",
    "    return datos_consolidados\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci칩n principal para leer un archivo Markdown, consolidar sus tablas\n",
    "    en un 칰nico archivo CSV.\n",
    "    \"\"\"\n",
    "    archivo_md = \"readme.md\"\n",
    "    archivo_csv_salida = \"casos_de_uso_consolidados.csv\"\n",
    "\n",
    "    try:\n",
    "        with open(archivo_md, \"r\", encoding=\"utf-8\") as f:\n",
    "            contenido = f.read()\n",
    "\n",
    "        datos_para_csv = extraer_y_transformar_tablas(contenido)\n",
    "\n",
    "        if len(datos_para_csv) <= 1:  # Solo contiene el encabezado\n",
    "            print(\n",
    "                f\"No se encontraron tablas de casos de uso v치lidas en '{archivo_md}'.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        with open(archivo_csv_salida, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(datos_para_csv)\n",
    "\n",
    "        print(f\"춰칄xito! Se procesaron las tablas y se guardaron en un 칰nico archivo:\")\n",
    "        print(f\"-> {archivo_csv_salida}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo '{archivo_md}' no fue encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri칩 un error inesperado: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932b60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (1825832334.py, line 335)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 335\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcatch:\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Starting Fiverr Trend Analysis workflow\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTION: SAFE DATA EXTRACTION\n",
    "# ==========================================\n",
    "def safe_extract(result, key=\"data\"):\n",
    "    if not isinstance(result, dict):\n",
    "        print(\n",
    "            f\"[{datetime.utcnow().isoformat()}] Warning: Expected dict, got {type(result)}\"\n",
    "        )\n",
    "        return result  # devolver tal cual si no es dict: permite listas/strings\n",
    "\n",
    "    data = result.get(key, result)  # si no est치 la clave, devolver todo el result\n",
    "\n",
    "    # caso nested: {\"data\": {\"data\": {...}}}\n",
    "    if isinstance(data, dict) and key in data and isinstance(data[key], (dict, list)):\n",
    "        print(f\"[{datetime.utcnow().isoformat()}] Debug: Found nested {key} structure\")\n",
    "        return data[key]\n",
    "\n",
    "    # permitir dict o list\n",
    "    if isinstance(data, (dict, list, str, int, float)):\n",
    "        return data\n",
    "\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: {key} is unexpected type: {type(data)}\"\n",
    "    )\n",
    "    return {}\n",
    "\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    # 1) try to find code fence JSON\n",
    "    if \"```json\" in text:\n",
    "        candidate = text.split(\"```json\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "    elif \"```\" in text:\n",
    "        candidate = text.split(\"```\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "    else:\n",
    "        # 2) heuristic: get substring between first { and last }\n",
    "        first = text.find(\"{\")\n",
    "        last = text.rfind(\"}\")\n",
    "        if first != -1 and last != -1 and last > first:\n",
    "            candidate = text[first : last + 1]\n",
    "        else:\n",
    "            candidate = text  # fallback\n",
    "\n",
    "    # try json.loads with multiple attempts / tolerant options\n",
    "    try:\n",
    "        return json.loads(candidate)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # try to fix common issues: replace single quotes, remove trailing commas (simple heuristics)\n",
    "        fixed = candidate.replace(\"'\", '\"')\n",
    "        fixed = re.sub(r\",\\s*}\", \"}\", fixed)\n",
    "        fixed = re.sub(r\",\\s*]\", \"]\", fixed)\n",
    "        try:\n",
    "            return json.loads(fixed)\n",
    "        except Exception:\n",
    "            raise Exception(\n",
    "                f\"Failed to parse LLM output as JSON. Error: {e}. Snippet: {candidate[:1000]}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: GET INPUTS FROM ENVIRONMENT\n",
    "# ==========================================\n",
    "buyer_name = os.environ.get(\"buyer_name\")\n",
    "niche = os.environ.get(\"niche\")\n",
    "keyword_list = os.environ.get(\"keyword_list\")\n",
    "geography = os.environ.get(\"geography\")\n",
    "timeframe = os.environ.get(\"timeframe\")\n",
    "\n",
    "if not all([buyer_name, niche, keyword_list, geography, timeframe]):\n",
    "    raise ValueError(\n",
    "        \"All inputs are required: buyer_name, niche, keyword_list, geography, timeframe\"\n",
    "    )\n",
    "\n",
    "keywords = [k.strip() for k in keyword_list.split(\",\") if k.strip()]\n",
    "\n",
    "if not keywords:\n",
    "    raise ValueError(\"keyword_list should have at least a valid keyword\")\n",
    "\n",
    "print(\n",
    "    f\"[{datetime.utcnow().isoformat()}] Inputs validated: {len(keywords)} keywords for {niche}\"\n",
    ")\n",
    "\n",
    "# Generate order ID\n",
    "order_id = f\"#{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: CREATE NOTION PAGE (Processing status)\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Creating Notion record\")\n",
    "\n",
    "notion_result, notion_error = run_composio_tool(\n",
    "    \"NOTION_CREATE_PAGE\",\n",
    "    {\n",
    "        \"parent_database_id\": \"2a3c26c2-89a3-8193-9792-d497f0c68065\",\n",
    "        \"properties\": {\n",
    "            \"Order ID\": {\"title\": [{\"text\": {\"content\": order_id}}]},\n",
    "            \"Client Name\": {\"rich_text\": [{\"text\": {\"content\": buyer_name}}]},\n",
    "            \"Niche\": {\"rich_text\": [{\"text\": {\"content\": niche}}]},\n",
    "            \"Keywords\": {\"rich_text\": [{\"text\": {\"content\": keyword_list}}]},\n",
    "            \"Geography\": {\"select\": {\"name\": geography}},\n",
    "            \"Timeframe\": {\"select\": {\"name\": timeframe}},\n",
    "            \"Status\": {\"select\": {\"name\": \"游댃 Processing\"}},\n",
    "            \"Date Received\": {\"date\": {\"start\": datetime.utcnow().isoformat()}},\n",
    "            \"Files\": {\"rich_text\": [{\"text\": {\"content\": \"\"}}]},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "if notion_error:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: Notion creation failed: {notion_error}\"\n",
    "    )\n",
    "    notion_page_id = None\n",
    "    notion_page_url = None\n",
    "else:\n",
    "    notion_data = safe_extract(notion_result)\n",
    "    notion_page_id = notion_data.get(\"id\")\n",
    "    notion_page_url = notion_data.get(\"url\")\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Notion record created: {notion_page_id}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: PARALLEL DATA COLLECTION (4 sources)\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Starting parallel data collection\")\n",
    "\n",
    "\n",
    "def fetch_trends():\n",
    "    result, error = run_composio_tool(\n",
    "        \"COMPOSIO_SEARCH_TRENDS\",\n",
    "        {\n",
    "            \"query\": f\"{niche} {keywords[0]}\",\n",
    "            \"timeframe\": timeframe,\n",
    "            \"geo\": geography if geography != \"Global\" else \"\",\n",
    "        },\n",
    "    )\n",
    "    return (\"trends\", result, error)\n",
    "\n",
    "\n",
    "def fetch_web():\n",
    "    result, error = run_composio_tool(\n",
    "        \"COMPOSIO_SEARCH_WEB\",\n",
    "        {\"query\": f\"{niche} market size trends {geography}\", \"num_results\": 10},\n",
    "    )\n",
    "    return (\"web\", result, error)\n",
    "\n",
    "\n",
    "def fetch_news():\n",
    "    result, error = run_composio_tool(\n",
    "        \"COMPOSIO_SEARCH_NEWS\",\n",
    "        {\n",
    "            \"query\": f\"{niche} industry news trends\",\n",
    "            \"num_results\": 10,\n",
    "            \"from_date\": (datetime.utcnow().replace(day=1)).isoformat(),\n",
    "        },\n",
    "    )\n",
    "    return (\"news\", result, error)\n",
    "\n",
    "\n",
    "def fetch_scholar():\n",
    "    result, error = run_composio_tool(\n",
    "        \"COMPOSIO_SEARCH_SCHOLAR\",\n",
    "        {\"query\": f\"{niche} consumer behavior trends\", \"num_results\": 5},\n",
    "    )\n",
    "    return (\"scholar\", result, error)\n",
    "\n",
    "\n",
    "# Execute all fetches in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [\n",
    "        executor.submit(fn)\n",
    "        for fn in [fetch_trends, fetch_web, fetch_news, fetch_scholar]\n",
    "    ]\n",
    "    results = {}\n",
    "    try:\n",
    "        for future in concurrent.futures.as_completed(futures, timeout=240):\n",
    "            try:\n",
    "                source, data, error = future.result()\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"[{datetime.utcnow().isoformat()}] Warning: fetch worker raised: {e}\"\n",
    "                )\n",
    "                continue\n",
    "            if error:\n",
    "                print(\n",
    "                    f\"[{datetime.utcnow().isoformat()}] Warning: {source} failed: {error}\"\n",
    "                )\n",
    "                results[source] = {}\n",
    "            else:\n",
    "                results[source] = safe_extract(data)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(\n",
    "            f\"[{datetime.utcnow().isoformat()}] Warning: Data collection timed out (240s). Proceeding with partial results.\"\n",
    "        )\n",
    "        # optionally cancel remaining futures\n",
    "        for f in futures:\n",
    "            if not f.done():\n",
    "                f.cancel()\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Data collection complete\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: AI SYNTHESIS (Your 6-task structure)\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Starting AI analysis\")\n",
    "\n",
    "\n",
    "def safe_truncate_json(data, max_chars=100000):\n",
    "    try:\n",
    "        json_str = json.dumps(data, indent=2, default=str)\n",
    "    except Exception:\n",
    "        # si no es serializable, devolver un resumen\n",
    "        return json.dumps(\n",
    "            {\"_summary\": \"non-serializable data\", \"type\": str(type(data))}\n",
    "        )\n",
    "\n",
    "    if len(json_str) <= max_chars:\n",
    "        return json_str\n",
    "\n",
    "    # Construir una versi칩n segura: conservar metadatos + top items si es lista/dict\n",
    "    if isinstance(data, list):\n",
    "        summary = {\n",
    "            \"_type\": \"list\",\n",
    "            \"length\": len(data),\n",
    "            \"sample\": data[:5],  # keep small sample\n",
    "        }\n",
    "        return json.dumps(summary, indent=2, default=str)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        keys = list(data.keys())[:20]\n",
    "        summary = {\n",
    "            \"_type\": \"dict\",\n",
    "            \"num_keys\": len(data),\n",
    "            \"keys_sample\": keys,\n",
    "            \"note\": \"full payload truncated for size\",\n",
    "        }\n",
    "        return json.dumps(summary, indent=2, default=str)\n",
    "\n",
    "    # fallback\n",
    "    return json_str[:max_chars] + \"\\n... (truncated)\"\n",
    "\n",
    "\n",
    "analysis_prompt = f\"\"\"You are a professional trend analyst. Analyze the following data and produce a structured report.\n",
    "\n",
    "Client: {buyer_name}\n",
    "Niche: {niche}\n",
    "Keywords: {keyword_list}\n",
    "Geography: {geography}\n",
    "Timeframe: {timeframe}\n",
    "\n",
    "Data collected:\n",
    "{safe_truncate_json(results, 100000)}\n",
    "\n",
    "Tasks (FOLLOW THIS EXACT STRUCTURE):\n",
    "\n",
    "1) Executive Summary: Give a 3-sentence summary of the current state of this trend.\n",
    "\n",
    "2) Keyword Trends: For EACH keyword ({keyword_list}), provide:\n",
    "   - Status: HOT / RISING / PLATEAU / DECLINING\n",
    "   - One-line rationale\n",
    "\n",
    "3) Forecasts: Produce 3 forecasts:\n",
    "   - 0-3 months: prediction, confidence (LOW/MED/HIGH), key metric to watch\n",
    "   - 3-12 months: prediction, confidence, key metric\n",
    "   - 12-36 months: prediction, confidence, key metric\n",
    "\n",
    "4) 90-Day Tactical Plan: Provide 3 prioritized actions:\n",
    "   - Priority 1 (Marketing): specific action + rationale\n",
    "   - Priority 2 (Product): specific action + rationale\n",
    "   - Priority 3 (Pricing): specific action + rationale\n",
    "\n",
    "5) Validation Checklist: Provide steps for the client to run themselves to validate this trend.\n",
    "\n",
    "6) Sources: List all data sources with URLs and 1-line explanation of what the data shows.\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"executive_summary\": \"3 sentences here\",\n",
    "  \"keyword_trends\": [\n",
    "    {{\"keyword\": \"keyword1\", \"status\": \"RISING\", \"rationale\": \"one line\"}}\n",
    "  ],\n",
    "  \"forecasts\": [\n",
    "    {{\"timeframe\": \"0-3 months\", \"prediction\": \"...\", \"confidence\": \"HIGH\", \"key_metric\": \"...\"}}\n",
    "  ],\n",
    "  \"tactical_90_day_plan\": [\n",
    "    {{\"priority\": 1, \"category\": \"Marketing\", \"action\": \"...\", \"rationale\": \"...\"}}\n",
    "  ],\n",
    "  \"validation_checklist\": [\"step 1\", \"step 2\", \"step 3\", ...],\n",
    "  \"data_sources\": [\n",
    "    {{\"source\": \"Google Trends\", \"url\": \"...\", \"explanation\": \"...\"}}\n",
    "  ],\n",
    "  \"trend_rating\": \"RISING\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "analysis_result, analysis_error = invoke_llm(analysis_prompt)\n",
    "\n",
    "if analysis_error:\n",
    "    raise Exception(f\"AI analysis failed: {analysis_error}\")\n",
    "\n",
    "# Parse JSON from LLM response using the new function\n",
    "try:\n",
    "    analysis = extract_json_from_text(analysis_result)\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] AI analysis completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] JSON parse error: {e}\")\n",
    "    raise Exception(f\"Failed to parse AI response as JSON: {analysis_result[:500]}\")\n",
    "\n",
    "# ==========================================\n",
    "# VALIDATION: Required fields in analysis\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Validating analysis fields\")\n",
    "\n",
    "required_fields = [\n",
    "    \"executive_summary\",\n",
    "    \"keyword_trends\",\n",
    "    \"forecasts\",\n",
    "    \"tactical_90_day_plan\",\n",
    "    \"validation_checklist\",\n",
    "    \"data_sources\",\n",
    "    \"trend_rating\",\n",
    "]\n",
    "\n",
    "for field in required_fields:\n",
    "    if field not in analysis or not analysis[field]:\n",
    "        raise Exception(f\"AI analysis missing required field: {field}\")\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] All required fields validated successfully\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: GENERATE CHARTS\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Generating charts\")\n",
    "try:\n",
    "    # Chart 1: Forecast Confidence Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    forecasts = analysis.get(\"forecasts\", [])\n",
    "    timeframes = [f[\"timeframe\"] for f in forecasts]\n",
    "    confidences = [\n",
    "        {\"LOW\": 0.3, \"MED\": 0.6, \"HIGH\": 0.9}.get(f[\"confidence\"], 0.5)\n",
    "        for f in forecasts\n",
    "    ]\n",
    "\n",
    "    bars = ax.bar(timeframes, confidences, color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"Confidence Level\", fontsize=12)\n",
    "    ax.set_xlabel(\"Timeframe\", fontsize=12)\n",
    "    ax.set_title(f\"Trend Forecast Confidence - {niche}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    for i, (bar, conf) in enumerate(zip(bars, confidences)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.02,\n",
    "            f'{forecasts[i][\"confidence\"]}',\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    chart1_path = \"/tmp/forecast_chart.png\"\n",
    "    plt.savefig(chart1_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Chart 2: Keyword Status Matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    keyword_trends = analysis.get(\"keyword_trends\", [])\n",
    "    keywords_chart = [kt[\"keyword\"] for kt in keyword_trends]\n",
    "    statuses = [kt[\"status\"] for kt in keyword_trends]\n",
    "\n",
    "    status_colors = {\n",
    "        \"HOT\": \"#FF4757\",\n",
    "        \"RISING\": \"#2ED573\",\n",
    "        \"PLATEAU\": \"#FFA502\",\n",
    "        \"DECLINING\": \"#747D8C\",\n",
    "    }\n",
    "\n",
    "    colors = [status_colors.get(s, \"#95A5A6\") for s in statuses]\n",
    "    y_pos = np.arange(len(keywords_chart))\n",
    "\n",
    "    bars = ax.barh(y_pos, [1] * len(keywords_chart), color=colors)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(keywords_chart)\n",
    "    ax.set_xlabel(\"Trend Status\", fontsize=12)\n",
    "    ax.set_title(f\"Keyword Trend Status - {niche}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    for i, (bar, status) in enumerate(zip(bars, statuses)):\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            bar.get_y() + bar.get_height() / 2.0,\n",
    "            status,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "            color=\"white\",\n",
    "            fontsize=11,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    chart2_path = \"/tmp/keyword_status_chart.png\"\n",
    "    plt.savefig(chart2_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Charts generated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Chart generation failed: {e}\")\n",
    "    raise Exception(f\"Failed to generate charts: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: GENERATE MARKDOWN REPORT\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Generating markdown report\")\n",
    "\n",
    "report_md = f\"\"\"# Trend Analysis Report: {niche}\n",
    "\n",
    "**Client:** {buyer_name}  \n",
    "**Order ID:** {order_id}  \n",
    "**Date:** {datetime.utcnow().strftime('%B %d, %Y')}  \n",
    "**Geography:** {geography}  \n",
    "**Timeframe:** {timeframe}\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "{analysis.get('executive_summary', 'N/A')}\n",
    "\n",
    "**Overall Trend Rating:** {analysis.get('trend_rating', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## Keyword Trend Analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for kt in analysis.get(\"keyword_trends\", []):\n",
    "    report_md += f\"### {kt['keyword']}\\n\"\n",
    "    report_md += f\"**Status:** {kt['status']}  \\n\"\n",
    "    report_md += f\"**Rationale:** {kt['rationale']}\\n\\n\"\n",
    "\n",
    "report_md += \"---\\n\\n## Market Forecasts\\n\\n\"\n",
    "\n",
    "for fc in analysis.get(\"forecasts\", []):\n",
    "    report_md += f\"### {fc['timeframe']}\\n\"\n",
    "    report_md += f\"**Prediction:** {fc['prediction']}  \\n\"\n",
    "    report_md += f\"**Confidence:** {fc['confidence']}  \\n\"\n",
    "    report_md += f\"**Key Metric to Watch:** {fc['key_metric']}\\n\\n\"\n",
    "\n",
    "report_md += \"---\\n\\n## 90-Day Tactical Action Plan\\n\\n\"\n",
    "\n",
    "for action in analysis.get(\"tactical_90_day_plan\", []):\n",
    "    report_md += f\"### Priority {action['priority']}: {action['category']}\\n\"\n",
    "    report_md += f\"**Action:** {action['action']}  \\n\"\n",
    "    report_md += f\"**Rationale:** {action['rationale']}\\n\\n\"\n",
    "\n",
    "report_md += \"---\\n\\n## Validation Checklist\\n\\n\"\n",
    "\n",
    "for i, step in enumerate(analysis.get(\"validation_checklist\", []), 1):\n",
    "    report_md += f\"{i}. {step}\\n\"\n",
    "\n",
    "report_md += \"\\n---\\n\\n## Data Sources\\n\\n\"\n",
    "\n",
    "for source in analysis.get(\"data_sources\", []):\n",
    "    report_md += f\"- **{source['source']}**: {source['explanation']}  \\n\"\n",
    "    if source.get(\"url\"):\n",
    "        report_md += f\"  [{source['url']}]({source['url']})\\n\"\n",
    "    report_md += \"\\n\"\n",
    "\n",
    "markdown_path = \"/tmp/trend_report.md\"\n",
    "with open(markdown_path, \"w\") as f:\n",
    "    f.write(report_md)\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Markdown report generated\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 7: CONVERT TO PDF\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Converting to PDF\")\n",
    "\n",
    "pdf_result, pdf_error = run_composio_tool(\"TEXT_TO_PDF\", {\"text\": report_md})\n",
    "\n",
    "if pdf_error:\n",
    "    raise Exception(f\"PDF conversion failed: {pdf_error}\")\n",
    "\n",
    "pdf_data = safe_extract(pdf_result)\n",
    "pdf_url_temp = (\n",
    "    pdf_data.get(\"pdf_url\")\n",
    "    or pdf_data.get(\"url\")\n",
    "    or pdf_data.get(\"file_url\")\n",
    "    or pdf_data.get(\"download_url\")\n",
    "    or pdf_data.get(\"s3_url\")\n",
    "    or \"\"\n",
    ")\n",
    "\n",
    "# O mejor a칰n, inspeccionar las claves\n",
    "if not pdf_url_temp:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Available PDF keys: {list(pdf_data.keys())}\"\n",
    "    )\n",
    "\n",
    "if not pdf_url_temp:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: No PDF URL found in response: {pdf_data}\"\n",
    "    )\n",
    "    raise Exception(\"Failed to get PDF URL from conversion result\")\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] PDF generated: {pdf_url_temp}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 8: UPLOAD TO GOOGLE DRIVE\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Uploading files to Google Drive\")\n",
    "\n",
    "\n",
    "def upload_file_to_google_drive(\n",
    "    local_file_path, folder_number, drive_filename=\"\", mimetype=\"\"\n",
    "):\n",
    "\n",
    "    # Define Google Drive folder IDs for different file types\n",
    "    DRIVE_FOLDERS = [\n",
    "        \"1kxZOQ3ZbOvQqu2U6xxKl_C5XrF795cL2\",  # charts & assets\n",
    "        \"1u3N35rlIOXJsAWEQT-dY_Zl-Tthpbr3T\",  # archive\n",
    "        \"15roNpcqV5cPe2EITbyAFFa9PKJlxK1oZ\",  # data & CSVs\n",
    "        \"1auHTHgYWev8H_1h9ngui6WgfEKI65kSo\",  # reports\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Step 1: Upload to Composio storage\n",
    "        temp_upload, upload_err = upload_local_file(local_file_path)\n",
    "\n",
    "        if upload_err:\n",
    "            return None, f\"Failed to upload to Composio storage: {upload_err}\"\n",
    "\n",
    "        # Use safe extraction for upload result\n",
    "        upload_data = (\n",
    "            safe_extract(temp_upload)\n",
    "            if isinstance(temp_upload, dict)\n",
    "            else temp_upload or {}\n",
    "        )\n",
    "        s3key = upload_data.get(\"key\")\n",
    "\n",
    "        if not s3key:\n",
    "            print(\n",
    "                f\"[{datetime.utcnow().isoformat()}] Warning: No s3key in upload response: {upload_data}\"\n",
    "            )\n",
    "            return None, \"Failed to get s3key from Composio storage upload\"\n",
    "\n",
    "        # Step 2: Upload from Composio storage to Google Drive\n",
    "        drive_result, drive_err = run_composio_tool(\n",
    "            \"GOOGLEDRIVE_UPLOAD_FILE\",\n",
    "            {\n",
    "                \"file_to_upload\": {\n",
    "                    \"name\": drive_filename,\n",
    "                    \"mimetype\": mimetype,\n",
    "                    \"s3key\": s3key,\n",
    "                },\n",
    "                \"folder_to_upload_to\": DRIVE_FOLDERS[folder_number],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if drive_err:\n",
    "            return None, f\"Failed to upload to Google Drive: {drive_err}\"\n",
    "\n",
    "        drive_data = safe_extract(drive_result)\n",
    "        drive_link = drive_data.get(\"webViewLink\")\n",
    "\n",
    "        if not drive_link:\n",
    "            print(\n",
    "                f\"[{datetime.utcnow().isoformat()}] Warning: No webViewLink in drive response: {drive_data}\"\n",
    "            )\n",
    "            return None, \"Failed to get Google Drive link from upload result\"\n",
    "\n",
    "        return drive_link, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error during upload: {str(e)}\"\n",
    "\n",
    "\n",
    "# Upload Markdown report to Drive\n",
    "markdown_url, markdown_err = upload_file_to_google_drive(\n",
    "    markdown_path, 3, f\"{order_id}_trend_report.md\", \"text/markdown\"\n",
    ")\n",
    "\n",
    "if markdown_err:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: Markdown upload error: {markdown_err}\"\n",
    "    )\n",
    "    markdown_url = \"\"\n",
    "else:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Markdown report uploaded successfully\")\n",
    "\n",
    "# Upload to Drive\n",
    "pdf_drive_url, pdf_drive_err = upload_file_to_google_drive(\n",
    "    pdf_url_temp, 3, f\"{order_id}_trend_report.pdf\", \"application/pdf\"\n",
    ")\n",
    "\n",
    "if pdf_drive_err:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: PDF Drive upload error: {pdf_drive_err}\"\n",
    "    )\n",
    "    pdf_drive_url = \"\"\n",
    "else:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] PDF report uploaded successfully\")\n",
    "\n",
    "# Upload charts\n",
    "chart1_url, err1 = upload_file_to_google_drive(\n",
    "    chart1_path, 0, f\"{order_id}_forecast_chart.png\", \"image/png\"\n",
    ")\n",
    "chart2_url, err2 = upload_file_to_google_drive(\n",
    "    chart2_path, 0, f\"{order_id}_keyword_status_chart.png\", \"image/png\"\n",
    ")\n",
    "\n",
    "if err1 or err2:\n",
    "    print(\n",
    "        f\"[{datetime.utcnow().isoformat()}] Warning: Chart upload errors: {err1} {err2}\"\n",
    "    )\n",
    "\n",
    "chart1_url = chart1_url if not err1 else \"\"\n",
    "chart2_url = chart2_url if not err2 else \"\"\n",
    "\n",
    "if not chart1_url or not chart2_url:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Warning: Some charts failed to upload\")\n",
    "\n",
    "# Create CSV files for each data source\n",
    "csv_urls = {}\n",
    "import csv\n",
    "\n",
    "for source_name, source_data in results.items():\n",
    "    csv_path = f\"/tmp/{order_id}_{source_name}_data.csv\"\n",
    "\n",
    "    try:\n",
    "        # Convert nested data to flat CSV format\n",
    "        if isinstance(source_data, dict):\n",
    "            # Handle dictionary data\n",
    "            with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Key\", \"Value\"])  # Header\n",
    "\n",
    "                def flatten_dict(d, parent_key=\"\", sep=\"_\"):\n",
    "                    items = []\n",
    "                    for k, v in d.items():\n",
    "                        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "                        if isinstance(v, dict):\n",
    "                            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "                        elif isinstance(v, list):\n",
    "                            for i, item in enumerate(v):\n",
    "                                if isinstance(item, dict):\n",
    "                                    items.extend(\n",
    "                                        flatten_dict(\n",
    "                                            item, f\"{new_key}_{i}\", sep=sep\n",
    "                                        ).items()\n",
    "                                    )\n",
    "                                else:\n",
    "                                    items.append((f\"{new_key}_{i}\", str(item)))\n",
    "                        else:\n",
    "                            items.append((new_key, str(v)))\n",
    "                    return dict(items)\n",
    "\n",
    "                flattened = flatten_dict(source_data)\n",
    "                for key, value in flattened.items():\n",
    "                    writer.writerow([key, value])\n",
    "\n",
    "        elif isinstance(source_data, list):\n",
    "            # Handle list data\n",
    "            with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if source_data and isinstance(source_data[0], dict):\n",
    "                    # List of dictionaries\n",
    "                    if source_data:\n",
    "                        writer.writerow(source_data[0].keys())  # Header\n",
    "                        for item in source_data:\n",
    "                            writer.writerow(item.values())\n",
    "                else:\n",
    "                    # Simple list\n",
    "                    writer.writerow([\"Index\", \"Value\"])\n",
    "                    for i, item in enumerate(source_data):\n",
    "                        writer.writerow([i, str(item)])\n",
    "        else:\n",
    "            # Handle simple data types\n",
    "            with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Source\", \"Data\"])\n",
    "                writer.writerow([source_name, str(source_data)])\n",
    "\n",
    "        # Upload the CSV file\n",
    "        csv_upload, csv_err = upload_file_to_google_drive(\n",
    "            csv_path, 2, f\"{order_id}_{source_name}_data.csv\", \"text/csv\"\n",
    "        )\n",
    "        if not csv_err:\n",
    "            csv_urls[f\"{source_name}_csv\"] = csv_upload or \"\"\n",
    "            print(\n",
    "                f\"[{datetime.utcnow().isoformat()}] {source_name} CSV uploaded successfully\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"[{datetime.utcnow().isoformat()}] Warning: {source_name} CSV upload failed: {csv_err}\"\n",
    "            )\n",
    "            csv_urls[f\"{source_name}_csv\"] = \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[{datetime.utcnow().isoformat()}] Warning: Failed to create {source_name} CSV: {e}\"\n",
    "        )\n",
    "        csv_urls[f\"{source_name}_csv\"] = \"\"\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] All files uploaded\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 9: UPDATE NOTION PAGE\n",
    "# ==========================================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Updating Notion record\")\n",
    "\n",
    "# Prepare files text with all Drive URLs\n",
    "files_text = []\n",
    "if pdf_drive_url:\n",
    "    files_text.append(f\"游늯 PDF Report: {pdf_drive_url}\")\n",
    "if markdown_url:\n",
    "    files_text.append(f\"游닇 Markdown Report: {markdown_url}\")\n",
    "if chart1_url:\n",
    "    files_text.append(f\"游늵 Forecast Chart: {chart1_url}\")\n",
    "if chart2_url:\n",
    "    files_text.append(f\"游늳 Keywords Chart: {chart2_url}\")\n",
    "\n",
    "# Add CSV files\n",
    "for csv_name, csv_url in csv_urls.items():\n",
    "    if csv_url:\n",
    "        source_name = csv_name.replace(\"_csv\", \"\").title()\n",
    "        files_text.append(f\"游늶 {source_name} Data: {csv_url}\")\n",
    "\n",
    "files_text_content = \"\\n\".join(files_text) if files_text else \"No files uploaded\"\n",
    "\n",
    "if notion_page_id:\n",
    "    update_result, update_error = run_composio_tool(\n",
    "        \"NOTION_UPDATE_PAGE\",\n",
    "        {\n",
    "            \"page_id\": notion_page_id,\n",
    "            \"properties\": {\n",
    "                \"Status\": {\"select\": {\"name\": \"游리 Waiting client review\"}},\n",
    "                \"Date Delivered\": {\"date\": {\"start\": datetime.utcnow().isoformat()}},\n",
    "                \"Trend Rating\": {\n",
    "                    \"select\": {\"name\": analysis.get(\"trend_rating\", \"RISING\")}\n",
    "                },\n",
    "                \"Executive Summary\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"text\": {\n",
    "                                \"content\": analysis.get(\"executive_summary\", \"\")[:2000]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"Files\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\n",
    "                            \"text\": {\n",
    "                                \"content\": files_text_content[\n",
    "                                    :2000\n",
    "                                ]  # Notion has character limits\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"Revenue\": {\"number\": 100},\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if update_error:\n",
    "        print(\n",
    "            f\"[{datetime.utcnow().isoformat()}] Warning: Notion update failed: {update_error}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"[{datetime.utcnow().isoformat()}] Notion record updated successfully\")\n",
    "\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Workflow completed successfully\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 10: RETURN OUTPUT\n",
    "# ==========================================\n",
    "output = {\n",
    "    \"order_id\": order_id,\n",
    "    \"executive_summary\": analysis.get(\"executive_summary\"),\n",
    "    \"keyword_trends\": analysis.get(\"keyword_trends\"),\n",
    "    \"forecasts\": analysis.get(\"forecasts\"),\n",
    "    \"tactical_90_day_plan\": analysis.get(\"tactical_90_day_plan\"),\n",
    "    \"validation_checklist\": analysis.get(\"validation_checklist\"),\n",
    "    \"data_sources\": analysis.get(\"data_sources\"),\n",
    "    \"drive_links\": {\n",
    "        \"pdf_report\": pdf_drive_url,  # Now pointing to Drive instead of temp URL\n",
    "        \"markdown_report\": markdown_url,  # New: Markdown report on Drive\n",
    "        \"chart_forecast\": chart1_url,\n",
    "        \"chart_keywords\": chart2_url,\n",
    "        \"complete_folder\": \"https://drive.google.com/drive/folders/1cwIdofoHLuozO48WhNsQc-NRker0ez2c\",\n",
    "        **csv_urls,  # Include all CSV URLs for each data source,\n",
    "    },\n",
    "    \"notion_page_url\": notion_page_url,\n",
    "    \"trend_rating\": analysis.get(\"trend_rating\"),\n",
    "}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637526b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
